{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b714a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: outlines in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (0.1.11)\n",
      "Requirement already satisfied: datasets in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (0.46.0)\n",
      "Requirement already satisfied: peft in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: click in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (8.1.8)\n",
      "Requirement already satisfied: pydantic in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (2.11.5)\n",
      "Requirement already satisfied: instructor in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: scikit-learn in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: wandb in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: interegular in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (3.1.6)\n",
      "Requirement already satisfied: lark in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (1.2.2)\n",
      "Requirement already satisfied: nest_asyncio in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (1.6.0)\n",
      "Requirement already satisfied: numpy in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (3.1.1)\n",
      "Requirement already satisfied: diskcache in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (5.6.3)\n",
      "Requirement already satisfied: referencing in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (4.24.0)\n",
      "Requirement already satisfied: requests in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (4.14.0)\n",
      "Requirement already satisfied: pycountry in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (20250622)\n",
      "Requirement already satisfied: torch in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (2.7.0)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from outlines) (0.1.26)\n",
      "Requirement already satisfied: filelock in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (0.32.4)\n",
      "Requirement already satisfied: packaging in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.11)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
      "Requirement already satisfied: psutil in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from torch->outlines) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from triton==3.3.0->torch->outlines) (80.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (0.16)\n",
      "Requirement already satisfied: jiter<0.11,>=0.6.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (0.8.2)\n",
      "Requirement already satisfied: mkdocs-material>=9.5.49 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (9.6.14)\n",
      "Requirement already satisfied: mkdocs>=1.6.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (1.6.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.70.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (1.86.0)\n",
      "Requirement already satisfied: pre-commit>=4.2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (4.2.0)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.7.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (9.1.2)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from instructor) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from jinja2->outlines) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.70.0->instructor) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.70.0->instructor) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.70.0->instructor) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.70.0->instructor) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.70.0->instructor) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.70.0->instructor) (3.10)\n",
      "Requirement already satisfied: certifi in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.70.0->instructor) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.70.0->instructor) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.70.0->instructor) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from requests->outlines) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from requests->outlines) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from rich<15.0.0,>=13.7.0->instructor) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from rich<15.0.0,>=13.7.0->instructor) (2.19.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: setproctitle in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.7.0->instructor) (0.1.2)\n",
      "Requirement already satisfied: ghp-import>=1.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (2.1.0)\n",
      "Requirement already satisfied: markdown>=3.3.6 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (3.8.2)\n",
      "Requirement already satisfied: mergedeep>=1.3.4 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (1.3.4)\n",
      "Requirement already satisfied: mkdocs-get-deps>=0.2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (0.2.0)\n",
      "Requirement already satisfied: pathspec>=0.11.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (0.12.1)\n",
      "Requirement already satisfied: pyyaml-env-tag>=0.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (1.1)\n",
      "Requirement already satisfied: watchdog>=2.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs>=1.6.1->instructor) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from ghp-import>=1.0->mkdocs>=1.6.1->instructor) (2.9.0.post0)\n",
      "Requirement already satisfied: babel~=2.10 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs-material>=9.5.49->instructor) (2.17.0)\n",
      "Requirement already satisfied: backrefs~=5.7.post1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs-material>=9.5.49->instructor) (5.9)\n",
      "Requirement already satisfied: colorama~=0.4 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs-material>=9.5.49->instructor) (0.4.6)\n",
      "Requirement already satisfied: mkdocs-material-extensions~=1.3 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs-material>=9.5.49->instructor) (1.3.1)\n",
      "Requirement already satisfied: paginate~=0.5 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs-material>=9.5.49->instructor) (0.5.7)\n",
      "Requirement already satisfied: pymdown-extensions~=10.2 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from mkdocs-material>=9.5.49->instructor) (10.16)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pre-commit>=4.2.0->instructor) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pre-commit>=4.2.0->instructor) (2.6.12)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pre-commit>=4.2.0->instructor) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pre-commit>=4.2.0->instructor) (20.31.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->ghp-import>=1.0->mkdocs>=1.6.1->instructor) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->outlines) (1.3.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit>=4.2.0->instructor) (0.3.9)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from jsonschema->outlines) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from jsonschema->outlines) (0.25.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/automl/KD-SLM/venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install outlines datasets transformers accelerate bitsandbytes peft click pydantic instructor scikit-learn wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f5259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting CaseHold evaluation of 3 models\n",
      "============================================================\n",
      "\n",
      "ü§ñ [1/3] Evaluating: bigscience/bloomz-560m\n",
      "----------------------------------------\n",
      "üöÄ Starting evaluation for: bigscience/bloomz-560m\n",
      "üìä Sample size: 50\n",
      "üíæ GPU Poor mode: True (4bit quantization)\n",
      "üìÅ Results will be saved to: /kaggle/working\n",
      "üìö Loading dataset...\n",
      "‚úÖ Dataset loaded: 50 samples\n",
      "ü§ñ Loading model...\n",
      "‚úÖ Hugging Face model loaded successfully\n",
      "üî§ Loading tokenizer...\n",
      "Tokenizer missing chat template. Applying default Llama 3 template.\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "‚ùå Error creating outlines model: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'BloomForCausalLM(\n",
      "  (transformer): BloomModel(\n",
      "    (word_embeddings): Embedding(250880, 1024)\n",
      "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x BloomBlock(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): BloomAttention(\n",
      "          (query_key_value): Linear4bit(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): BloomMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
      "          (gelu_impl): BloomGelu()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
      ")'.\n",
      "‚ùå Failed to evaluate bigscience/bloomz-560m\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "ü§ñ [2/3] Evaluating: facebook/opt-350m\n",
      "----------------------------------------\n",
      "üöÄ Starting evaluation for: facebook/opt-350m\n",
      "üìä Sample size: 50\n",
      "üíæ GPU Poor mode: True (4bit quantization)\n",
      "üìÅ Results will be saved to: /kaggle/working\n",
      "üìö Loading dataset...\n",
      "‚úÖ Dataset loaded: 50 samples\n",
      "ü§ñ Loading model...\n",
      "‚úÖ Hugging Face model loaded successfully\n",
      "üî§ Loading tokenizer...\n",
      "Tokenizer missing chat template. Applying default Llama 3 template.\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "‚ùå Error creating outlines model: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")'.\n",
      "‚ùå Failed to evaluate facebook/opt-350m\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "ü§ñ [3/3] Evaluating: EleutherAI/pythia-410m\n",
      "----------------------------------------\n",
      "üöÄ Starting evaluation for: EleutherAI/pythia-410m\n",
      "üìä Sample size: 50\n",
      "üíæ GPU Poor mode: True (4bit quantization)\n",
      "üìÅ Results will be saved to: /kaggle/working\n",
      "üìö Loading dataset...\n",
      "‚úÖ Dataset loaded: 50 samples\n",
      "ü§ñ Loading model...\n",
      "‚úÖ Hugging Face model loaded successfully\n",
      "üî§ Loading tokenizer...\n",
      "Tokenizer missing chat template. Applying default Llama 3 template.\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "‚ùå Error creating outlines model: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 1024)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (query_key_value): Linear4bit(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
      "          (dense_4h_to_h): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
      ")'.\n",
      "‚ùå Failed to evaluate EleutherAI/pythia-410m\n",
      "üßπ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import datasets\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "import outlines\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "from outlines.models import transformers as from_transformers\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# This Enum is not needed for the generative evaluation but is kept as requested.\n",
    "class CaseholdAnswer(Enum):\n",
    "    holding_0 = \"0\"\n",
    "    holding_1 = \"1\"\n",
    "    holding_2 = \"2\"\n",
    "    holding_3 = \"3\"\n",
    "    holding_4 = \"4\"\n",
    "\n",
    "def evaluate_model(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    sample_size=100,\n",
    "    dataset_path=\"MothMalone/SLMS-KD-Benchmarks\",\n",
    "    dataset_name=\"casehold\",\n",
    "    split=True,\n",
    "    gpu_poor=True,\n",
    "    quant_mode=\"4bit\",\n",
    "    output_dir=\"/kaggle/working\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a single model on Casehold dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting evaluation for: {model_name}\")\n",
    "    print(f\"üìä Sample size: {sample_size}\")\n",
    "    print(f\"üíæ GPU Poor mode: {gpu_poor} ({quant_mode} quantization)\")\n",
    "    \n",
    "    # Create output filenames\n",
    "    model_short_name = model_name.split('/')[-1]\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if gpu_poor:\n",
    "        base_name = f\"{model_short_name}_{quant_mode}_{timestamp}\"\n",
    "    else:\n",
    "        base_name = f\"{model_short_name}_full_{timestamp}\"\n",
    "    \n",
    "    log_file = os.path.join(output_dir, f\"metrics_{base_name}.csv\")\n",
    "    results_file = os.path.join(output_dir, f\"detailed_{base_name}.csv\")\n",
    "    summary_file = os.path.join(output_dir, f\"summary_{base_name}.txt\")\n",
    "    json_file = os.path.join(output_dir, f\"results_{base_name}.json\")\n",
    "    \n",
    "    print(f\"üìÅ Results will be saved to: {output_dir}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"üìö Loading dataset...\")\n",
    "    try:\n",
    "        ds = datasets.load_dataset(dataset_path, dataset_name)\n",
    "        if split:\n",
    "            ds_split = 'test' if 'test' in ds else 'validation'\n",
    "            ds = ds[ds_split].select(range(min(sample_size, len(ds[ds_split]))))\n",
    "        else:\n",
    "            ds = ds['train'].select(range(min(sample_size, len(ds['train']))))\n",
    "        print(f\"‚úÖ Dataset loaded: {len(ds)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Load model from transformers\n",
    "    print(\"ü§ñ Loading model...\")\n",
    "    model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
    "    if gpu_poor:\n",
    "        if quant_mode == \"4bit\":\n",
    "            model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "        else:  # 8bit\n",
    "            model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "    try:\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "        print(\"‚úÖ Hugging Face model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"üî§ Loading tokenizer...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        if tokenizer.chat_template is None:\n",
    "            print(\"Tokenizer missing chat template. Applying default Llama 3 template.\")\n",
    "            tokenizer.chat_template = (\n",
    "                \"{% for message in messages %}{% if message['role'] == 'system' %}\"\n",
    "                \"{{'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>'}}\"\n",
    "                \"{% elif message['role'] == 'user' %}\"\n",
    "                \"{{'<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>'}}\"\n",
    "                \"{% elif message['role'] == 'assistant' %}\"\n",
    "                \"{{'<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>'}}\"\n",
    "                \"{% endif %}{% endfor %}{% if add_generation_prompt %}\"\n",
    "                \"{{'<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}{% endif %}\"\n",
    "            )\n",
    "        print(\"‚úÖ Tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Create outlines model by wrapping the loaded transformers model\n",
    "    try:\n",
    "        # FIX: Using the function call you requested.\n",
    "        # Make repr(hf_model) return a safe string (Outlines uses repr for repo_id)\n",
    "        safe_name = model_name.replace(\"/\", \"-\")\n",
    "        import types\n",
    "        hf_model.__repr__ = types.MethodType(lambda self: safe_name, hf_model)\n",
    "        model = from_transformers(hf_model, tokenizer)\n",
    "        print(\"‚úÖ Outlines model created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating outlines model: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize tracking\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    detailed_results = []\n",
    "    metrics_history = []\n",
    "\n",
    "    print(\"\\nüîÑ Starting evaluation...\")\n",
    "    \n",
    "    # Process each sample\n",
    "    for i, row in enumerate(ds):\n",
    "        print(f\"Processing sample {i+1}/{len(ds)}\", end=\" - \")\n",
    "        \n",
    "        # Create a simple prompt that matches the fine-tuning task.\n",
    "        user_prompt = row['citing_prompt']\n",
    "        chat = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # The choices for the model are the full text of the holdings.\n",
    "        choices = [row[f'holding_{j}'] for j in range(5)]\n",
    "        true_label_text = row[f\"holding_{row['label']}\"]\n",
    "\n",
    "        try:\n",
    "            # Constrain generation to the full text of the choices.\n",
    "            generator = outlines.generate.choice(model, choices)\n",
    "            predicted_label_text = generator(prompt, max_tokens=256)\n",
    "            \n",
    "            # FIX: Find the best matching holding using fuzzy matching\n",
    "            from difflib import SequenceMatcher\n",
    "            \n",
    "            best_match_score = 0\n",
    "            predicted_label = -1  # if no good match\n",
    "            \n",
    "            for j in range(5):\n",
    "                # Calculate similarity between predicted text and each holding\n",
    "                similarity = SequenceMatcher(None, predicted_label_text.lower().strip(), \n",
    "                                           row[f'holding_{j}'].lower().strip()).ratio()\n",
    "                if similarity > best_match_score:\n",
    "                    best_match_score = similarity\n",
    "                    predicted_label = j\n",
    "            \n",
    "            # Compare numerical labels\n",
    "            true_label = row['label']  # This should be 0-4\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(predicted_label)\n",
    "\n",
    "            if (predicted_label != -1):\n",
    "                is_correct = true_label == predicted_label\n",
    "                print(f\"True: {true_label}, Pred: {predicted_label} (sim: {best_match_score:.3f}), ‚úÖ\" if is_correct else f\"True: {true_label}, Pred: {predicted_label} (sim: {best_match_score:.3f}), ‚ùå\")\n",
    "                \n",
    "                detailed_results.append({\n",
    "                    'sample_id': i,\n",
    "                    'prompt': user_prompt,\n",
    "                    'true_holding': true_label,\n",
    "                    'predicted_holding': predicted_label,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "            else:\n",
    "                print(\"Bad generation, Skip example.\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)[:100]}...\")\n",
    "            y_true.append(true_label_text)\n",
    "            y_pred.append(\"ERROR\")\n",
    "            detailed_results.append({'sample_id': i, 'prompt': user_prompt, 'true_holding': true_label_text, 'predicted_holding': 'ERROR', 'correct': False, 'error': str(e)})\n",
    "\n",
    "        # Calculate metrics every 10 samples\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(ds):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            metrics_row = {\n",
    "                \"model\": model_name,\n",
    "                \"sample\": i + 1,\n",
    "                \"accuracy\": round(accuracy * 100, 3),\n",
    "                \"f1_macro\": round(f1_macro * 100, 3),\n",
    "                \"f1_weighted\": round(f1_weighted * 100, 3)\n",
    "            }\n",
    "            metrics_history.append(metrics_row)\n",
    "            pd.DataFrame(metrics_history).to_csv(log_file, index=False)\n",
    "            print(f\"üìä Progress: Acc={accuracy*100:.1f}%\")\n",
    "\n",
    "    # --- Final calculations and reporting ---\n",
    "    print(\"\\n‚úÖ Evaluation complete.\")\n",
    "    pd.DataFrame(detailed_results).to_csv(results_file, index=False)\n",
    "    print(f\"‚úÖ Detailed results saved to: {results_file}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL RESULTS FOR {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üéØ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"üìà F1 Score (Weighted): {f1_weighted * 100:.2f}%\")\n",
    "    print(f\"üìà F1 Score (Macro): {f1_macro * 100:.2f}%\")\n",
    "    \n",
    "    all_labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "    print(f\"üî¢ Confusion Matrix:\\n{pd.DataFrame(cm, index=all_labels, columns=all_labels).to_string()}\")\n",
    "\n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"dataset\": f\"{dataset_path}/{dataset_name}\",\n",
    "        \"total_samples\": sample_size,\n",
    "        \"valid_samples\": len(valid_pairs),\n",
    "        \"error_samples\": sample_size - len(valid_pairs),\n",
    "        \"success_rate\": len(valid_pairs) / sample_size * 100,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": float(accuracy * 100),\n",
    "            \"precision_weighted\": float(precision * 100),\n",
    "            \"recall_weighted\": float(recall * 100),\n",
    "            \"f1_weighted\": float(f1 * 100),\n",
    "            \"f1_macro\": float(f1_macro * 100)\n",
    "        },\n",
    "        \"confusion_matrix\": conf_matrix.tolist(),\n",
    "        \"label_distribution\": {\n",
    "            \"true_labels\": np.bincount(y_true_array, minlength=5).tolist(),  # 5 holdings\n",
    "            \"predicted_labels\": np.bincount(y_pred_array, minlength=5).tolist()\n",
    "        },\n",
    "        \"configuration\": {\n",
    "            \"gpu_poor\": gpu_poor,\n",
    "            \"quantization\": quant_mode if gpu_poor else None,\n",
    "            \"evaluation_time\": str(pd.Timestamp.now())\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save JSON results\n",
    "    try:\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"‚úÖ JSON results saved to: {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not save JSON results: {e}\")\n",
    "    \n",
    "    # Save text summary\n",
    "    try:\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"=== CASEHOLD EVALUATION RESULTS ===\\n\")\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"Dataset: {dataset_path}/{dataset_name}\\n\")\n",
    "            f.write(f\"Total Samples: {sample_size}\\n\")\n",
    "            f.write(f\"Valid Predictions: {len(valid_pairs)}\\n\")\n",
    "            f.write(f\"Success Rate: {len(valid_pairs)/sample_size*100:.2f}%\\n\\n\")\n",
    "            \n",
    "            f.write(f\"=== PERFORMANCE METRICS ===\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy * 100:.3f}%\\n\")\n",
    "            f.write(f\"Precision (Weighted): {precision * 100:.3f}%\\n\")\n",
    "            f.write(f\"Recall (Weighted): {recall * 100:.3f}%\\n\")\n",
    "            f.write(f\"F1 Score (Weighted): {f1 * 100:.3f}%\\n\")\n",
    "            f.write(f\"F1 Score (Macro): {f1_macro * 100:.3f}%\\n\\n\")\n",
    "            \n",
    "            f.write(f\"=== CONFUSION MATRIX ===\\n\")\n",
    "            f.write(f\"Rows: True Labels, Columns: Predicted Labels\\n\")\n",
    "            f.write(f\"Order: [holding_0=0, holding_1=1, holding_2=2, holding_3=3, holding_4=4]\\n\")\n",
    "            f.write(f\"{conf_matrix}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"=== LABEL DISTRIBUTION ===\\n\")\n",
    "            f.write(f\"True Labels: {np.bincount(y_true_array, minlength=5).tolist()}\\n\")\n",
    "            f.write(f\"Predicted Labels: {np.bincount(y_pred_array, minlength=5).tolist()}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"=== CONFIGURATION ===\\n\")\n",
    "            f.write(f\"GPU Poor Mode: {gpu_poor}\\n\")\n",
    "            f.write(f\"Quantization: {quant_mode if gpu_poor else 'None'}\\n\")\n",
    "            f.write(f\"Evaluation Time: {pd.Timestamp.now()}\\n\")\n",
    "            \n",
    "        print(f\"‚úÖ Summary saved to: {summary_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not save summary: {e}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return {\"model_name\": model_name, \"metrics\": {\"accuracy\": accuracy, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weighted}}\n",
    "\n",
    "def evaluate_all_models(\n",
    "    models=None,\n",
    "    sample_size=100,\n",
    "    output_dir=\"/kaggle/working\",\n",
    "    gpu_poor=True,\n",
    "    quant_mode=\"4bit\"\n",
    "):\n",
    "    \"\"\"Evaluate multiple models sequentially on CaseHold dataset\"\"\"\n",
    "    if models is None:\n",
    "        models = [\n",
    "            \"bigscience/bloomz-560m\",\n",
    "            \"facebook/opt-350m\",\n",
    "            \"EleutherAI/pythia-410m\"\n",
    "        ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"üöÄ Starting CaseHold evaluation of {len(models)} models\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, model in enumerate(models, 1):\n",
    "        print(f\"\\nü§ñ [{i}/{len(models)}] Evaluating: {model}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            result = evaluate_model(\n",
    "                model_name=model,\n",
    "                sample_size=sample_size,\n",
    "                output_dir=output_dir,\n",
    "                gpu_poor=gpu_poor,\n",
    "                quant_mode=quant_mode\n",
    "            )\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "                print(f\"‚úÖ Successfully evaluated {model}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to evaluate {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unhandled error evaluating {model}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"üßπ GPU memory cleared\")\n",
    "    \n",
    "     # Create comparison summary\n",
    "    if all_results:\n",
    "        print(f\"\\nüìä CASEHOLD COMPARISON SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for result in all_results:\n",
    "            comparison_data.append({\n",
    "                'Model': result['model_name'].split('/')[-1],\n",
    "                'Valid Samples': result['valid_samples'],\n",
    "                'Success Rate': f\"{result['success_rate']:.1f}%\",\n",
    "                'Accuracy': f\"{result['metrics']['accuracy']:.2f}%\",\n",
    "                'F1 Weighted': f\"{result['metrics']['f1_weighted']:.2f}%\",\n",
    "                'F1 Macro': f\"{result['metrics']['f1_macro']:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Save comparison\n",
    "        try:\n",
    "            comparison_file = os.path.join(output_dir, f\"casehold_comparison_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "            comparison_df.to_csv(comparison_file, index=False)\n",
    "            print(f\"\\n‚úÖ Comparison saved to: {comparison_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not save comparison: {e}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    custom_models = [\n",
    "        \"bigscience/bloomz-560m\",\n",
    "        \"facebook/opt-350m\",\n",
    "        \"EleutherAI/pythia-410m\"\n",
    "    ]\n",
    "    results = evaluate_all_models(models=custom_models, sample_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c87cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: 'function' object has no attribute '__file__'. Did you mean: '__le__'?\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import outlines; print(outlines.models.transformers.__file__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97055b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
