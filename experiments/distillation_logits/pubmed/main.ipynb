{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af742d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexuanan/KD-SLM/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e27382",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONTEXT = \"\"\"\n",
    "PubMedQA is a dataset and a task that involves Question Answering (QA) using scientific literature from PubMed, which is a free resource that contains millions of articles related to life sciences and biomedical research. PubMedQA specifically focuses on using abstracts and passages from PubMed articles to answer medical and scientific questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\" : {\n",
    "        \"name\": \"MothMalone/SLMS-KD-Benchmarks\"\n",
    "    }, \n",
    "    \"models\": {\n",
    "        \"teacher\": \"meta-llama/Llama-2-13b\",\n",
    "        \"student\": \"meta-llama/Llama-3.2-1B\"\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "    \"max_length\": 4096,\n",
    "    \"chat_template\" : \"\"\"\n",
    "            {% for message in messages %}\n",
    "            {% if loop.first and messages[0]['role'] != 'system' %}\n",
    "                {{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
    "            {% endif %}\n",
    "            {{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "            {% endfor %}\n",
    "            {% if add_generation_prompt %}\n",
    "            {{ '<|im_start|>assistant\\n' }}\n",
    "            {% endif %}\n",
    "    \"\"\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"save_steps\": 1000,\n",
    "        \"logging_steps\": 1,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.05,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"resume_from_checkpoint\": None,\n",
    "        \"fp16\": False,\n",
    "        \"bf16\": True\n",
    "    },\n",
    "    \"model_config\": {\n",
    "        \"use_flash_attention\": False\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76650eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
       "    num_rows: 700\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(config['dataset']['name'], 'pubmedqa')\n",
    "dataset = dataset['train'].select(range(700))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d5cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tokenizer = AutoTokenizer.from_pretrained(config[\"models\"][\"student\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a1aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizer(name_or_path='/home/lexuanan/.cache/huggingface/hub/models--meta-llama--Llama-2-13b/snapshots/5a3ad81c857aaf765c7a229a449490745a9004c9', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "# Set the path to where the model is stored after download\n",
    "model_path = \"/home/lexuanan/.cache/huggingface/hub/models--meta-llama--Llama-2-13b/snapshots/5a3ad81c857aaf765c7a229a449490745a9004c9\"\n",
    "\n",
    "# Load the tokenizer\n",
    "teacher_tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Verify that it loads correctly\n",
    "print(teacher_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08a9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(row):\n",
    "    question =  f\"\"\"\n",
    "    {DATASET_CONTEXT}\n",
    "    Given the following question from the PubMedQA dataset: \n",
    "    question: {row['question']}, \n",
    "    with these data:\n",
    "    context: {row['context']}, \n",
    "    long_answer: {row['long_answer']}, \n",
    "    Please provide the final decision based on these data and provide the final_decision. It is either\n",
    "    \"yes\" or \"no\" or \"maybe\".\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"question\" : question \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958421fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(apply_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ccf152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['pubid','context', 'long_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb1f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "student_tokenizer.pad_token = student_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9ee17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(row):\n",
    "    return student_tokenizer(row[\"question\"], truncation=True, max_length=config[\"tokenizer\"][\"max_length\"], padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848f9f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['final_decision', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 700\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=8, remove_columns=[\"question\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42e2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"torch_dtype\": torch.bfloat16}\n",
    "if config[\"model_config\"][\"use_flash_attention\"]:\n",
    "    model_kwargs[\"attn_implementation\"] = \"flash_attention_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856df604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_logits(student_logits, teacher_logits):\n",
    "    student_size, teacher_size = student_logits.size(-1), teacher_logits.size(-1)\n",
    "    if student_size != teacher_size:\n",
    "        pad_size = abs(student_size - teacher_size)\n",
    "        pad_tensor = torch.zeros((*teacher_logits.shape[:-1], pad_size), dtype=teacher_logits.dtype, device=teacher_logits.device)\n",
    "        return (torch.cat([student_logits, pad_tensor], dim=-1), teacher_logits) if student_size < teacher_size else (student_logits, torch.cat([teacher_logits, pad_tensor], dim=-1))\n",
    "    return student_logits, teacher_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35297724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"torch_dtype\": torch.bfloat16}\n",
    "if config[\"model_config\"][\"use_flash_attention\"]:\n",
    "    model_kwargs[\"attn_implementation\"] = \"flash_attention_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27bc9724",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/lexuanan/.cache/huggingface/hub/models--meta-llama--Llama-2-13b/snapshots/5a3ad81c857aaf765c7a229a449490745a9004c9.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m teacher_tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the entire model from the directory (not just one .pth file)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Check if the model loaded successfully\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(teacher_model)\n",
      "File \u001b[0;32m~/KD-SLM/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/KD-SLM/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4422\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4413\u001b[0m     gguf_file\n\u001b[1;32m   4414\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4416\u001b[0m ):\n\u001b[1;32m   4417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4420\u001b[0m     )\n\u001b[0;32m-> 4422\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4424\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4435\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4439\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4441\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4442\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/KD-SLM/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:976\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    972\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    973\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    974\u001b[0m         )\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    977\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    978\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    979\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    980\u001b[0m         )\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m    982\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/lexuanan/.cache/huggingface/hub/models--meta-llama--Llama-2-13b/snapshots/5a3ad81c857aaf765c7a229a449490745a9004c9."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Set the model path\n",
    "model_path = \"/home/lexuanan/.cache/huggingface/hub/models--meta-llama--Llama-2-13b/snapshots/5a3ad81c857aaf765c7a229a449490745a9004c9\"\n",
    "\n",
    "# Load the tokenizer\n",
    "teacher_tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the entire model from the directory (not just one .pth file)\n",
    "teacher_model = LlamaForCausalLM.from_pretrained(model_path, **model_kwargs)\n",
    "\n",
    "# Check if the model loaded successfully\n",
    "print(teacher_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80959043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3be42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
